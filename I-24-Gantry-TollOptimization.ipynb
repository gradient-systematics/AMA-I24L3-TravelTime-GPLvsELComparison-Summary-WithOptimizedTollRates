{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600f1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install geopandas numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da8a93",
   "metadata": {},
   "source": [
    "# interactive Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6162d4",
   "metadata": {},
   "source": [
    "## Dict - I24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dee807",
   "metadata": {},
   "source": [
    "### Gantry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90935d70",
   "metadata": {},
   "source": [
    "#### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39a2ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ml = \"\"\"\n",
    "## N_-_leg_11_-_I-840_to_SR102\n",
    "207904\n",
    "## N_-_leg_10_-_SR102_to_SR266\n",
    "207852, 207600, 207601, 207602, 207603, 207604, 207805\n",
    "## N_-_leg_9_-_SR266_to_Waldron\n",
    "207605, 207606, 207607, 207916, 207608, 207609, 207610, 207611, 207612, 207613, 207614, 207615, 207806, 207599\n",
    "## N_-_leg_8_-_Waldron_to_SR171\n",
    "207616, 207909, 207617, 207618, 207619, 207620, 207621, 207622, 207623, 207624, 207625, 207626, 207627, 207628, 207629,\n",
    "## N_-_leg_7_-_SR171_to_SR254\n",
    "207630, 207631, 207632, 207633, 207634, 207635, 207636, 207637, 207638, 207856, 207854, 207639, 207640, 207641, 207807\n",
    "## N_-_leg_6_-_SR254_to_Haywood_Ln\n",
    "207642, 207643, 207812, 207644, 207645, 207646, 207647, 207648, 207649, 207843\n",
    "## N_-_leg_5_-_Haywood_Ln_to_SR255\n",
    "207655, 207870, 207654, 207653, 207652, 207862, 207846, 207650, 207651\n",
    "## N_-_leg_4_-_SR255_to_I-440\n",
    "207656, 207866, 207868, 207867, 207657, 207658, 207659, 207818, 207660, 207661, 207662, 207664, 207801, 207802, 207826, 207663, 207800, 207668, 207667, 207666, 207665\n",
    "## N_-_leg_3_-_I-440_to_Split\n",
    "207677, 207676, 207671, 207669\n",
    "## N_-_leg_2_-_Split_to_SR155North\n",
    "207794, 207940, 207799, 207930\n",
    "## N_-_leg_1_-_Split_to_Fesslers\n",
    "207776, 207830\n",
    "\n",
    "\n",
    "## S_-_leg_11_-_I-840_to_SR102\n",
    "207905\n",
    "## S_-_leg_10_-_SR102_to_SR266\n",
    "207754, 207755, 207756, 207757, 207758, 207759, 207760, 207761, 207762, 207763, 207764, 207765, 207766, 207767, 207768, 207769, 207770, 207771, 207772, 207773, 207774, 207775\n",
    "## S_-_leg_9_-_SR266_to_Waldron\n",
    "207741, 207742, 207912, 207743, 207744, 207745, 207746, 207747, 207748, 207749, 207750, 207751, 207915, 207752, 207753, \n",
    "## S_-_leg_8_-_Waldron_to_SR171\n",
    "207733, 207734, 207735, 207736, 207737, 207738, 207739, 207740, 207908\n",
    "## S_-_leg_7_-_SR171_to_SR254\n",
    "207715, 207816, 207716, 207717, 207718, 207719, 207720, 207721, 207722, 207723, 207724, 207857, 207725, 207726, 207727, 207728, 207729, 207730, 207731, 207732,\n",
    "## S_-_leg_6_-_SR254_to_Haywood_Ln\n",
    "207702, 207703, 207704, 207860, 207705, 207706, 207707, 207708, 207709, 207710, 207711, 207712, 207713, 207714\n",
    "## S_-_leg_5_-_Haywood_Ln_to_SR255\n",
    "207865, 207695, 207696, 207869, 207697, 207698, 207699, 207850, 207700, 207701\n",
    "## S_-_leg_4_-_SR255_to_I-440\n",
    "207694, 207864, 207693, 207692, 207691, 207690, 207689, 207688, 207687, 207686, 207685, 207684, 207841, 207683, 207817, 207804, 207682, 207839, 207803, 207902, 207681, 207680, 207837\n",
    "## S_-_leg_3_-_I-440_to_Split\n",
    "207679, 207793, 207795, 207678, 207786\n",
    "## S_-_leg_2_-_Split_SR155North\n",
    "207791, 207792, 207872, 207931\n",
    "## S_-_leg_1_-_Split_to_Fesslers\n",
    "207779, 207780, 207782\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5286a",
   "metadata": {},
   "source": [
    "#### GPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "969b198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_gpl = \"\"\"\n",
    "## N_-_leg_11_-_I-840_to_SR102\n",
    "169617, 207941, 169619, 169620, 207110\n",
    "## N_-_leg_10_-_SR102_to_SR266\n",
    "169618, 205754, 205755, 169621, 169622\n",
    "## N_-_leg_9_-_SR266_to_Waldron\n",
    "169623, 169624, 169625\n",
    "## N_-_leg_8_-_Waldron_to_SR171\n",
    "169626, 141484, 141485, \n",
    "## N_-_leg_7_-_SR171_to_SR254\n",
    "141486, 207096, 207095, 141487, 141488, 141489\n",
    "## N_-_leg_6_-_SR254_to_Haywood_Ln\n",
    "141489, 141490, 141491\n",
    "## N_-_leg_5_-_Haywood_Ln_to_SR255\n",
    "141492, 141493, 207161, 141494, 207143\n",
    "## N_-_leg_4_-_SR255_to_I-440\n",
    "207152, 141495, 141496, 141497, 207268, 199424, 141499\n",
    "## N_-_leg_3_-_I-440_to_Split\n",
    "141500, 141501, 141501, 141502, 180062\n",
    "## N_-_leg_2_-_Split_to_SR155North\n",
    "207101, 141760, 141761\n",
    "## N_-_leg_1_-_Split_to_Fesslers\n",
    "198404, 207106\n",
    "\n",
    "\n",
    "## S_-_leg_11_-_I-840_to_SR102\n",
    "169501, 169500, 169499, 169498, 169497, 169496, 169495, 169494, 205757, 207091, 169489, 169490, 169492, 169493\n",
    "## S_-_leg_10_-_SR102_to_SR266\n",
    "169491, 205756, 169488, 169487, 169486, 169485, 169484, 169483, 169482, 169480, 169479, 169478, 169477, 169476,\n",
    "## S_-_leg_9_-_SR266_to_Waldron\n",
    "169475, 169474, 169473, 169472, 169471, 169470, 169469, 169468, 169467, 169466, 169465, 169464, 169463, 169462, 169461, 169460, 169459, 169458, 169457, 169456, 169455, 169454, 169453, 169452\n",
    "## S_-_leg_8_-_Waldron_to_SR171\n",
    "169451, 169450, 169449, 169448, 169447, 169446, 141483, 141482, 141481, 141480\n",
    "## S_-_leg_7_-_SR171_to_SR254\n",
    "141479, 141478, 141477, 141476, 141475, 141474, 141473, 207097, 141472, 141471, 141470, 141469, 141468, 141467, 141466, 141465, 141464, 141463, 141462, 141461, 141460, 141459, 141458, 141457, 141456\n",
    "## S_-_leg_6_-_SR254_to_Haywood_Ln\n",
    "141455, 141454, 141453, 141452, 141451, 141450, 141449, 141448, 141447, 141446, 141445, 141444, 141443, 141442, 141441\n",
    "## S_-_leg_5_-_Haywood_Ln_to_SR255\n",
    "141440, 141439, 141438, 141437, 141436, 141435, 141434, 207159, 141433, 141432, 207138, 141431\n",
    "## S_-_leg_4_-_SR255_to_I-440\n",
    "141430, 141429, 141428, 141427, 141426, 141425, 141424, 141423, 141422, 141421, 141420, 141419, 141418, 141417, 141416, 141415, 141414, 141413, 141412, 141411, 141410, 141409, 141408, 141407\n",
    "## S_-_leg_3_-_I-440_to_Split\n",
    "199058, 141405, 141406, 141392, 141393, 141394, 178783, 141395, 141396, 141397, 141398, 141399, 141400\n",
    "## S_-_leg_2_-_Split_SR155North\n",
    "207587, 141690, 141689, 141688, 141684, 141686, 141687,141685\n",
    "## S_-_leg_1_-_Split_Fesslers\n",
    "141680, 207105, 141681, 141682, 141683\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285c61a",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fcf45da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "def parse_leg_string(text: str) -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Convert a “## …” leg description string into\n",
    "    {section_title: [list, of, ints], …}.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"##\\s*(.+?)\\s*\\n\"        # capture the section title (everything after '## ')\n",
    "        r\"([\\d,\\s]+)\",            # capture the following line(s) of numbers\n",
    "        re.DOTALL                 # allow the numbers to span multiple lines if needed\n",
    "    )\n",
    "\n",
    "    result: Dict[str, List[int]] = {}\n",
    "    for title, numbers in pattern.findall(text):\n",
    "        # Split by comma, strip spaces, convert to int\n",
    "        num_list = [int(n.strip()) for n in numbers.split(\",\") if n.strip()]\n",
    "        result[title.strip()] = num_list\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb7c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_legs_ml = parse_leg_string(string_ml)\n",
    "dict_legs_gpl = parse_leg_string(string_gpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e669bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_legs = parse_leg_string(string_ml)\n",
    "# dict_legs = parse_leg_string(string_gpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b25f7",
   "metadata": {},
   "source": [
    "## Map Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "267c5ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shapefile...\n",
      "Found 91916 total links\n",
      "Found 228 relevant links from dictionary\n",
      "✓ Interactive map saved to: c:\\Git_projects\\I-24-Gantry-TollOptimization\\Output\\interactive_leg_map.html\n",
      "✓ Map centered on relevant links from dictionary\n",
      "✓ Features:\n",
      "  - Sidebar with clickable leg names (color-coded)\n",
      "  - Links use geometry from shapefile\n",
      "  - Each leg uses one of 4 main colors\n",
      "  - White circles at link endpoints\n",
      "  - Link ID labels on each link\n",
      "  - Multiple legs can be active simultaneously\n",
      "  - Zoom in/out functionality enabled\n",
      "  - Select All / Clear All buttons\n",
      "  - Click on link labels to select them\n",
      "  - Copy selected link IDs to clipboard\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "import folium\n",
    "from folium.features import DivIcon\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "Interactive I-24 Leg Map with Sidebar Selection\n",
    "─────────────────────────────────────────────────────────\n",
    "Single map with clickable leg names in sidebar\n",
    "Links displayed with coordinates from shapefile\n",
    "\"\"\"\n",
    "\n",
    "# --------------- 1. USER SETTINGS ------------------------------------\n",
    "# SHP_PATH = r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\OUTPUT-20250527\\NoBuilt\\TransCAD\\shapefiles\\NoBuilt_newVOT_2024\\AM_NoBuilt_newVOT_2024.shp\"\n",
    "# SHP_PATH = r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\OUTPUT-20250527\\Geographical file\\Master_Network_Links 2025-07-15.shp\"\n",
    "# SHP_PATH = r\"C:/Users/aligr/Gradient Systematics, LL/AMA-I24SL3 - Documents/06-TDM/Model Result/OUTPUT-20250527/NoBuilt/TransCAD/shapefiles/NoBuilt_newVOT_2024/AM_NoBuilt_newVOT_2024.shp\"\n",
    "# SHP_PATH = r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\OUTPUT-20250527\\Geographical file\\Master_Network_Links 2025-07-15.shp\"\n",
    "# SHP_PATH = r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\OUTPUT-20250527\\Base SHP\\TDOT_NewBaseDesign.shp\"\n",
    "SHP_PATH = r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\OUTPUT-20250527\\Geographical file\\Master_Network_Links 2025-07-15.shp\"\n",
    "OUTPUT_DIR = Path.cwd() / \"Output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Settings\n",
    "LINK_WIDTH = 6\n",
    "LABEL_FONT_SIZE = \"14px\"\n",
    "ENDPOINT_RADIUS = 8\n",
    "\n",
    "# 4 main colors for legs\n",
    "MAIN_COLORS = ['#002060', '#8B0000', '#006400', '#4B0082']\n",
    "\n",
    "# Read the main shapefile\n",
    "print(\"Loading shapefile...\")\n",
    "links_gdf = (\n",
    "    gpd.read_file(SHP_PATH, columns=[\"ID\", \"geometry\", \"RTE_NME\", \"DIR\"])\n",
    "       .to_crs(4326)\n",
    ")\n",
    "# links_gdf = links_gdf[links_gdf[\"RTE_NME\"] == \"I-24 CHOICE LANES\"]\n",
    "\n",
    "print(f\"Found {len(links_gdf)} total links\")\n",
    "\n",
    "# Get only the links that are in the dictionary\n",
    "all_link_ids = []\n",
    "for leg_links in dict_legs.values():\n",
    "    all_link_ids.extend(leg_links)\n",
    "\n",
    "relevant_links = links_gdf[links_gdf['ID'].isin(all_link_ids)].copy()\n",
    "print(f\"Found {len(relevant_links)} relevant links from dictionary\")\n",
    "\n",
    "# Center the map on the relevant links\n",
    "if len(relevant_links) > 0:\n",
    "    bounds = relevant_links.total_bounds\n",
    "    center_lat = (bounds[1] + bounds[3]) / 2\n",
    "    center_lon = (bounds[0] + bounds[2]) / 2\n",
    "else:\n",
    "    # Fallback center\n",
    "    center_lat, center_lon = 36.1627, -86.7816\n",
    "\n",
    "# Create the map\n",
    "m = folium.Map(\n",
    "    location=(center_lat, center_lon),\n",
    "    tiles=\"CartoDB positron\",\n",
    "    zoom_start=12,\n",
    "    control_scale=True,\n",
    ")\n",
    "\n",
    "# Add title\n",
    "title_html = \"\"\"\n",
    "<div style=\"position: fixed; \n",
    "            top: 10px; left: 50%; transform: translateX(-50%);\n",
    "            width: auto; height: 40px; \n",
    "            background-color: white; border:2px solid grey; z-index:9999; \n",
    "            font-size:18px; font-weight: bold; padding: 10px;\n",
    "            text-align: center; border-radius: 5px;\">\n",
    "    Interactive I-24 Leg Selection Map\n",
    "</div>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Create sidebar with leg selection buttons\n",
    "sidebar_html = \"\"\"\n",
    "<div id=\"legSidebar\" style=\"position: fixed; \n",
    "                            top: 70px; left: 10px; \n",
    "                            width: 320px; \n",
    "                            background-color: white; \n",
    "                            border: 2px solid #333; \n",
    "                            border-radius: 5px; \n",
    "                            box-shadow: 0 2px 10px rgba(0,0,0,0.3);\n",
    "                            z-index: 9999; \n",
    "                            font-family: Arial, sans-serif;\n",
    "                            max-height: 80vh;\n",
    "                            overflow-y: auto;\">\n",
    "    <div style=\"background-color: #f0f0f0; padding: 10px; border-bottom: 1px solid #ccc;\">\n",
    "        <h3 style=\"margin: 0; font-size: 16px; color: #333;\">Select Legs to Display</h3>\n",
    "    </div>\n",
    "    <div style=\"padding: 10px;\">\n",
    "        <div style=\"margin-bottom: 10px;\">\n",
    "            <button id=\"selectAllBtn\" style=\"background: #4CAF50; color: white; border: none; \n",
    "                                           padding: 8px 12px; border-radius: 4px; cursor: pointer;\n",
    "                                           font-size: 12px; margin-right: 5px;\">\n",
    "                Select All\n",
    "            </button>\n",
    "            <button id=\"clearAllBtn\" style=\"background: #f44336; color: white; border: none; \n",
    "                                          padding: 8px 12px; border-radius: 4px; cursor: pointer;\n",
    "                                          font-size: 12px;\">\n",
    "                Clear All\n",
    "            </button>\n",
    "        </div>\n",
    "\"\"\"\n",
    "\n",
    "# Add buttons for each leg with color indicators\n",
    "for i, leg_name in enumerate(dict_legs.keys()):\n",
    "    clean_name = leg_name.replace('_', ' ').replace('-', ' ')\n",
    "    color = MAIN_COLORS[i % len(MAIN_COLORS)]\n",
    "    \n",
    "    sidebar_html += f\"\"\"\n",
    "        <div style=\"margin-bottom: 8px;\">\n",
    "            <button class=\"leg-button\" \n",
    "                    data-leg=\"{leg_name}\"\n",
    "                    data-color=\"{color}\"\n",
    "                    style=\"width: 100%; \n",
    "                           padding: 10px; \n",
    "                           background-color: #e0e0e0; \n",
    "                           border: 1px solid #ccc; \n",
    "                           border-radius: 4px; \n",
    "                           cursor: pointer;\n",
    "                           font-size: 13px;\n",
    "                           text-align: left;\n",
    "                           transition: background-color 0.2s;\n",
    "                           display: flex;\n",
    "                           align-items: center;\">\n",
    "                <div style=\"width: 20px; height: 20px; background-color: {color}; \n",
    "                           border-radius: 3px; margin-right: 10px; border: 1px solid #333;\"></div>\n",
    "                {clean_name}\n",
    "            </button>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "sidebar_html += \"\"\"\n",
    "    </div>\n",
    "    <div style=\"padding: 10px; border-top: 1px solid #ccc; background-color: #f9f9f9;\">\n",
    "        <div style=\"font-size: 12px; color: #666;\">\n",
    "            Active Legs: <span id=\"activeLegCount\">0</span>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "m.get_root().html.add_child(folium.Element(sidebar_html))\n",
    "\n",
    "# Add selection info box\n",
    "info_box_html = \"\"\"\n",
    "<div id=\"selectedLinksBox\"\n",
    "     style=\"position:fixed;top:70px;right:10px;z-index:9999;\n",
    "            background:#fff;padding:10px;border:2px solid #333;\n",
    "            border-radius:5px;box-shadow:0 2px 10px rgba(0,0,0,0.3);\n",
    "            font-family:Arial;font-size:14px;width:280px;max-height:400px;\n",
    "            overflow-y:auto;\">\n",
    "  <div style=\"display:flex;justify-content:space-between;align-items:center;margin-bottom:8px;\">\n",
    "    <b style=\"color:#333;\">Selected Links:</b>\n",
    "    <button id=\"copyLinks\" \n",
    "            style=\"background:#4CAF50;color:white;border:none;padding:4px 8px;\n",
    "                   border-radius:3px;cursor:pointer;font-size:12px;\">\n",
    "      Copy\n",
    "    </button>\n",
    "  </div>\n",
    "  <div id=\"linksList\" style=\"max-height:200px;overflow-y:auto;\n",
    "                              border:1px solid #ddd;padding:8px;\n",
    "                              background:#f9f9f9;border-radius:3px;\n",
    "                              font-family:monospace;font-size:12px;\n",
    "                              line-height:1.4;\">\n",
    "    Click on link labels to select them...\n",
    "  </div>\n",
    "  <div style=\"margin-top:8px;font-size:12px;color:#666;\">\n",
    "    Selected: <span id=\"linkCount\">0</span> links\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(info_box_html))\n",
    "\n",
    "# Create feature groups for each leg (initially empty)\n",
    "leg_groups = {}\n",
    "for leg_name in dict_legs.keys():\n",
    "    leg_group = folium.FeatureGroup(name=leg_name, show=False)\n",
    "    leg_group.add_to(m)\n",
    "    leg_groups[leg_name] = leg_group\n",
    "\n",
    "# Prepare leg data for JavaScript\n",
    "leg_data_for_js = {}\n",
    "for i, (leg_name, link_ids) in enumerate(dict_legs.items()):\n",
    "    leg_links = relevant_links[relevant_links['ID'].isin(link_ids)].copy()\n",
    "    color = MAIN_COLORS[i % len(MAIN_COLORS)]\n",
    "    \n",
    "    links_data = []\n",
    "    for _, row in leg_links.iterrows():\n",
    "        coords = [(pt[1], pt[0]) for pt in row.geometry.coords]  # Convert to lat,lon\n",
    "        \n",
    "        # Get endpoints\n",
    "        start_point = coords[0]\n",
    "        end_point = coords[-1]\n",
    "        \n",
    "        # Get midpoint for label\n",
    "        mid_geom = row.geometry.interpolate(0.5, normalized=True)\n",
    "        mid_point = [mid_geom.y, mid_geom.x]\n",
    "        \n",
    "        links_data.append({\n",
    "            'id': int(row.ID),\n",
    "            'coords': coords,\n",
    "            'start_point': start_point,\n",
    "            'end_point': end_point,\n",
    "            'mid_point': mid_point,\n",
    "            'route': str(row.RTE_NME) if row.RTE_NME else '',\n",
    "            'direction': str(row.DIR) if row.DIR else ''\n",
    "        })\n",
    "    \n",
    "    leg_data_for_js[leg_name] = {\n",
    "        'color': color,\n",
    "        'links': links_data\n",
    "    }\n",
    "\n",
    "# Add JavaScript for interactivity\n",
    "js_code = f\"\"\"\n",
    "<script>\n",
    "let activeLegGroups = {{}};\n",
    "let selectedLinks = [];\n",
    "let legData = {json.dumps(leg_data_for_js)};\n",
    "let linkLayers = {{}};\n",
    "\n",
    "function updateActiveLegCount() {{\n",
    "    const count = Object.keys(activeLegGroups).length;\n",
    "    document.getElementById('activeLegCount').textContent = count;\n",
    "}}\n",
    "\n",
    "function updateSelectedLinks() {{\n",
    "    const linksList = document.getElementById('linksList');\n",
    "    const linkCount = document.getElementById('linkCount');\n",
    "    \n",
    "    if (selectedLinks.length === 0) {{\n",
    "        linksList.innerHTML = 'Click on link labels to select them...';\n",
    "        linksList.style.color = '#999';\n",
    "    }} else {{\n",
    "        linksList.innerHTML = selectedLinks.join(', ');\n",
    "        linksList.style.color = '#333';\n",
    "    }}\n",
    "    \n",
    "    linkCount.textContent = selectedLinks.length;\n",
    "}}\n",
    "\n",
    "function addLegToMap(legName) {{\n",
    "    const legInfo = legData[legName];\n",
    "    if (!legInfo) return;\n",
    "    \n",
    "    const layerGroup = L.layerGroup();\n",
    "    linkLayers[legName] = layerGroup;\n",
    "    \n",
    "    legInfo.links.forEach(function(link) {{\n",
    "        // Add link polyline\n",
    "        const polyline = L.polyline(link.coords, {{\n",
    "            color: legInfo.color,\n",
    "            weight: {LINK_WIDTH},\n",
    "            opacity: 0.8\n",
    "        }}).bindTooltip(`Link ID: ${{link.id}} | Route: ${{link.route}} | Dir: ${{link.direction}}`);\n",
    "        \n",
    "        layerGroup.addLayer(polyline);\n",
    "        \n",
    "        // Add endpoint circles\n",
    "        const startCircle = L.circleMarker(link.start_point, {{\n",
    "            radius: {ENDPOINT_RADIUS},\n",
    "            color: legInfo.color,\n",
    "            fillColor: 'white',\n",
    "            fillOpacity: 1,\n",
    "            weight: 2\n",
    "        }});\n",
    "        \n",
    "        const endCircle = L.circleMarker(link.end_point, {{\n",
    "            radius: {ENDPOINT_RADIUS},\n",
    "            color: legInfo.color,\n",
    "            fillColor: 'white',\n",
    "            fillOpacity: 1,\n",
    "            weight: 2\n",
    "        }});\n",
    "        \n",
    "        layerGroup.addLayer(startCircle);\n",
    "        layerGroup.addLayer(endCircle);\n",
    "        \n",
    "        // Add link ID label\n",
    "        const labelMarker = L.marker(link.mid_point, {{\n",
    "            icon: L.divIcon({{\n",
    "                html: `<div class=\"link-label\" \n",
    "                           data-link-id=\"${{link.id}}\"\n",
    "                           style=\"font-size:{LABEL_FONT_SIZE};\n",
    "                                  background-color:rgba(255,255,255,0.95);\n",
    "                                  padding:3px 6px;\n",
    "                                  border:1px solid #333;\n",
    "                                  border-radius:3px;\n",
    "                                  font-weight:bold;\n",
    "                                  text-align:center;\n",
    "                                  cursor:pointer;\n",
    "                                  box-shadow: 0 1px 3px rgba(0,0,0,0.3);\n",
    "                                  transition:background-color 0.2s;\"\n",
    "                           onmouseover=\"this.style.backgroundColor='rgba(200,200,200,0.95)'\"\n",
    "                           onmouseout=\"this.style.backgroundColor='rgba(255,255,255,0.95)'\"\n",
    "                           onclick=\"toggleLinkSelection(${{link.id}})\">${{link.id}}</div>`,\n",
    "                className: 'link-label-marker',\n",
    "                iconSize: [30, 20],\n",
    "                iconAnchor: [15, 10]\n",
    "            }})\n",
    "        }});\n",
    "        \n",
    "        layerGroup.addLayer(labelMarker);\n",
    "    }});\n",
    "    \n",
    "    layerGroup.addTo({m.get_name()});\n",
    "}}\n",
    "\n",
    "function removeLegFromMap(legName) {{\n",
    "    if (linkLayers[legName]) {{\n",
    "        {m.get_name()}.removeLayer(linkLayers[legName]);\n",
    "        delete linkLayers[legName];\n",
    "    }}\n",
    "}}\n",
    "\n",
    "function toggleLegDisplay(legName) {{\n",
    "    const button = document.querySelector(`button[data-leg=\"${{legName}}\"]`);\n",
    "    const color = button.getAttribute('data-color');\n",
    "    \n",
    "    if (activeLegGroups[legName]) {{\n",
    "        // Hide leg\n",
    "        removeLegFromMap(legName);\n",
    "        delete activeLegGroups[legName];\n",
    "        button.style.backgroundColor = '#e0e0e0';\n",
    "        button.style.color = '#333';\n",
    "    }} else {{\n",
    "        // Show leg\n",
    "        addLegToMap(legName);\n",
    "        activeLegGroups[legName] = true;\n",
    "        button.style.backgroundColor = color;\n",
    "        button.style.color = 'white';\n",
    "    }}\n",
    "    \n",
    "    updateActiveLegCount();\n",
    "}}\n",
    "\n",
    "function selectAllLegs() {{\n",
    "    Object.keys(legData).forEach(function(legName) {{\n",
    "        if (!activeLegGroups[legName]) {{\n",
    "            toggleLegDisplay(legName);\n",
    "        }}\n",
    "    }});\n",
    "}}\n",
    "\n",
    "function clearAllLegs() {{\n",
    "    Object.keys(activeLegGroups).forEach(function(legName) {{\n",
    "        toggleLegDisplay(legName);\n",
    "    }});\n",
    "}}\n",
    "\n",
    "function toggleLinkSelection(linkId) {{\n",
    "    const index = selectedLinks.indexOf(linkId);\n",
    "    if (index > -1) {{\n",
    "        selectedLinks.splice(index, 1);\n",
    "    }} else {{\n",
    "        selectedLinks.push(linkId);\n",
    "    }}\n",
    "    updateSelectedLinks();\n",
    "}}\n",
    "\n",
    "function copySelectedLinks() {{\n",
    "    if (selectedLinks.length === 0) return;\n",
    "    \n",
    "    const textToCopy = selectedLinks.join(', ');\n",
    "    \n",
    "    if (navigator.clipboard) {{\n",
    "        navigator.clipboard.writeText(textToCopy);\n",
    "    }} else {{\n",
    "        const textArea = document.createElement('textarea');\n",
    "        textArea.value = textToCopy;\n",
    "        document.body.appendChild(textArea);\n",
    "        textArea.select();\n",
    "        document.execCommand('copy');\n",
    "        document.body.removeChild(textArea);\n",
    "    }}\n",
    "}}\n",
    "\n",
    "// Event listeners\n",
    "document.addEventListener('DOMContentLoaded', function() {{\n",
    "    // Leg button listeners\n",
    "    document.querySelectorAll('.leg-button').forEach(function(button) {{\n",
    "        button.addEventListener('click', function() {{\n",
    "            const legName = this.getAttribute('data-leg');\n",
    "            toggleLegDisplay(legName);\n",
    "        }});\n",
    "        \n",
    "        // Hover effects\n",
    "        button.addEventListener('mouseenter', function() {{\n",
    "            if (!activeLegGroups[this.getAttribute('data-leg')]) {{\n",
    "                this.style.backgroundColor = '#d0d0d0';\n",
    "            }}\n",
    "        }});\n",
    "        \n",
    "        button.addEventListener('mouseleave', function() {{\n",
    "            if (!activeLegGroups[this.getAttribute('data-leg')]) {{\n",
    "                this.style.backgroundColor = '#e0e0e0';\n",
    "            }}\n",
    "        }});\n",
    "    }});\n",
    "    \n",
    "    // Control button listeners\n",
    "    document.getElementById('selectAllBtn').addEventListener('click', selectAllLegs);\n",
    "    document.getElementById('clearAllBtn').addEventListener('click', clearAllLegs);\n",
    "    document.getElementById('copyLinks').addEventListener('click', copySelectedLinks);\n",
    "    \n",
    "    updateActiveLegCount();\n",
    "    updateSelectedLinks();\n",
    "}});\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "m.get_root().html.add_child(folium.Element(js_code))\n",
    "\n",
    "# Save the map\n",
    "output_path = OUTPUT_DIR / \"interactive_leg_map.html\"\n",
    "m.save(output_path)\n",
    "\n",
    "print(f\"✓ Interactive map saved to: {output_path}\")\n",
    "print(f\"✓ Map centered on relevant links from dictionary\")\n",
    "print(f\"✓ Features:\")\n",
    "print(f\"  - Sidebar with clickable leg names (color-coded)\")\n",
    "print(f\"  - Links use geometry from shapefile\")\n",
    "print(f\"  - Each leg uses one of 4 main colors\")\n",
    "print(f\"  - White circles at link endpoints\")\n",
    "print(f\"  - Link ID labels on each link\")\n",
    "print(f\"  - Multiple legs can be active simultaneously\")\n",
    "print(f\"  - Zoom in/out functionality enabled\")\n",
    "print(f\"  - Select All / Clear All buttons\")\n",
    "print(f\"  - Click on link labels to select them\")\n",
    "print(f\"  - Copy selected link IDs to clipboard\")\n",
    "\n",
    "# ── 1) Normalize types ──────────────────────────────────────────────\n",
    "links_gdf[\"ID\"] = links_gdf[\"ID\"].astype(int)\n",
    "ids_in_shp   = set(links_gdf[\"ID\"])\n",
    "ids_in_dict  = {int(i) for leg in dict_legs.values() for i in leg}\n",
    "\n",
    "# ── 2) What’s missing? ──────────────────────────────────────────────\n",
    "missing_in_shp = sorted(ids_in_dict - ids_in_shp)          # in dict, not in sh\n",
    "if missing_in_shp:\n",
    "    print(\"⚠ Links referenced in dict_legs but NOT found in shapefile:\")\n",
    "    print(missing_in_shp)\n",
    "\n",
    "# Optional: per‑leg detail\n",
    "for leg_name, link_ids in dict_legs.items():\n",
    "    miss = sorted(set(map(int, link_ids)) - ids_in_shp)\n",
    "    if miss:\n",
    "        print(f\"  • {leg_name}: missing {len(miss)} -> {miss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9d27424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Links referenced in dict_legs but NOT found in shapefile:\n",
      "[141680]\n",
      "  • E - leg 5 - 24 to Fesslers: missing 1 -> [141680]\n",
      "  • S - leg 3 - Fesslers to 24: missing 1 -> [141680]\n"
     ]
    }
   ],
   "source": [
    "# ── 1) Normalize types ──────────────────────────────────────────────\n",
    "links_gdf[\"ID\"] = links_gdf[\"ID\"].astype(int)\n",
    "ids_in_shp   = set(links_gdf[\"ID\"])\n",
    "ids_in_dict  = {int(i) for leg in dict_legs.values() for i in leg}\n",
    "\n",
    "# ── 2) What’s missing? ──────────────────────────────────────────────\n",
    "missing_in_shp = sorted(ids_in_dict - ids_in_shp)          # in dict, not in sh\n",
    "if missing_in_shp:\n",
    "    print(\"⚠ Links referenced in dict_legs but NOT found in shapefile:\")\n",
    "    print(missing_in_shp)\n",
    "\n",
    "# Optional: per‑leg detail\n",
    "for leg_name, link_ids in dict_legs.items():\n",
    "    miss = sorted(set(map(int, link_ids)) - ids_in_shp)\n",
    "    if miss:\n",
    "        print(f\"  • {leg_name}: missing {len(miss)} -> {miss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83a939",
   "metadata": {},
   "source": [
    "# Concatenate the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c223cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------- helper -----------------------------------------------------------\n",
    "def parse_key(key: str):\n",
    "    \"\"\"\n",
    "    Split a dictionary key like 'N_-_leg_5_-_SR102_to_SR266' into\n",
    "      • dir_code : 'NB', 'WB', 'SB', 'EB', …\n",
    "      • leg_part : 'leg_5_-_SR102_to_SR266'\n",
    "    \"\"\"\n",
    "    dir_part, leg_part = key.split('_-_', 1)\n",
    "    dir_part = dir_part.strip()\n",
    "    dir_code = dir_part + 'B' if len(dir_part) == 1 else dir_part\n",
    "    return dir_code, leg_part\n",
    "\n",
    "\n",
    "# ---------- build rows for both lane types ----------------------------------\n",
    "rows = []\n",
    "\n",
    "for lane_dict, lane_type in [\n",
    "    (dict_legs_ml,  \"ML\"),   # managed lanes\n",
    "    (dict_legs_gpl, \"GPL\"),  # general-purpose lanes\n",
    "]:\n",
    "    for key, id_list in lane_dict.items():\n",
    "        dir_code, leg_part = parse_key(key)\n",
    "\n",
    "        rows.extend(\n",
    "            {\n",
    "                \"direction\": dir_code,   # NB, WB, …\n",
    "                \"leg\":       leg_part,   # e.g. leg_5_-_SR102_to_SR266\n",
    "                \"id\":        link_id,    # numeric link ID\n",
    "                \"Type\":      lane_type   # ML or GPL\n",
    "            }\n",
    "            for link_id in id_list\n",
    "        )\n",
    "\n",
    "# ---------- final DataFrame --------------------------------------------------\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b9748",
   "metadata": {},
   "source": [
    "# Toll Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f02b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year  direction\n",
      "2024  EB           11\n",
      "      WB           11\n",
      "2035  EB           11\n",
      "      WB           11\n",
      "2045  EB           11\n",
      "      WB           11\n",
      "Name: Time_Period, dtype: int64\n",
      "direction     EB  WB\n",
      "Year segment        \n",
      "2024 1        11  11\n",
      "     2        11  11\n",
      "     3        11  11\n",
      "     4        11  11\n",
      "     5        11  11\n",
      "     6        11  11\n",
      "     7        11  11\n",
      "     8        11  11\n",
      "     9        11  11\n",
      "     10       11  11\n",
      "2035 1        11  11\n",
      "     2        11  11\n",
      "     3        11  11\n",
      "     4        11  11\n",
      "     5        11  11\n",
      "     6        11  11\n",
      "     7        11  11\n",
      "     8        11  11\n",
      "     9        11  11\n",
      "     10       11  11\n",
      "2045 1        11  11\n",
      "     2        11  11\n",
      "     3        11  11\n",
      "     4        11  11\n",
      "     5        11  11\n",
      "     6        11  11\n",
      "     7        11  11\n",
      "     8        11  11\n",
      "     9        11  11\n",
      "     10       11  11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "excel_path = r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\ATC_Model_Result\\Input\\Iteration 2 Scenario 1\\AMA-I24L3-OptimumTollRates.xlsx\"\n",
    "\n",
    "# --- helpers -----------------------------------------------------------------\n",
    "def _to_num(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip()\n",
    "    # remove $ and commas/spaces\n",
    "    s = re.sub(r\"[\\$,]\", \"\", s)\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def _is_year_token(x):\n",
    "    if pd.isna(x):\n",
    "        return False\n",
    "    s = str(x).strip()\n",
    "    return s in {\"2024\", \"2035\", \"2045\"}\n",
    "\n",
    "# --- read raw sheet (no headers to preserve layout/merged cells) --------------\n",
    "raw = pd.read_excel(excel_path, sheet_name=0, header=None, dtype=object)\n",
    "\n",
    "# locate the starting row of each table (cell A? contains the year)\n",
    "year_starts = [i for i, v in raw[0].items() if _is_year_token(v)]\n",
    "year_starts.sort()\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "for idx, start in enumerate(year_starts):\n",
    "    year = int(str(raw.iat[start, 0]).strip())\n",
    "    hdr1 = start + 1  # row with 'Segment | EB ... | WB ...'\n",
    "    hdr2 = start + 2  # row with time-period labels under EB/WB\n",
    "    data_start = start + 3\n",
    "\n",
    "    # Build column map: list of (col_index, direction, time_period)\n",
    "    colmap = []\n",
    "    cur_dir = None\n",
    "    ncols = raw.shape[1]\n",
    "\n",
    "    for j in range(1, ncols):\n",
    "        v1 = raw.iat[hdr1, j]  # may carry 'EB'/'WB' under merged cells\n",
    "        if isinstance(v1, str) and v1.strip().upper() in {\"EB\", \"WB\"}:\n",
    "            cur_dir = v1.strip().upper()\n",
    "\n",
    "        v2 = raw.iat[hdr2, j]\n",
    "        if pd.isna(v2):\n",
    "            continue\n",
    "        tp = str(v2).strip().upper()\n",
    "        if cur_dir in {\"EB\", \"WB\"} and tp:\n",
    "            colmap.append((j, cur_dir, tp))\n",
    "\n",
    "    # Iterate data rows until a blank segment or next year block\n",
    "    # Stop at next year start if present\n",
    "    next_start = year_starts[idx + 1] if idx + 1 < len(year_starts) else raw.shape[0]\n",
    "    for r in range(data_start, next_start):\n",
    "        seg_raw = raw.iat[r, 0]\n",
    "        if pd.isna(seg_raw):\n",
    "            break\n",
    "        seg = pd.to_numeric(str(seg_raw).strip(), errors=\"coerce\")\n",
    "        if pd.isna(seg):\n",
    "            break\n",
    "        seg = int(seg)\n",
    "\n",
    "        for (j, d, tp) in colmap:\n",
    "            val = _to_num(raw.iat[r, j])\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            rows_out.append(\n",
    "                {\n",
    "                    \"Year\": year,\n",
    "                    \"segment\": seg,\n",
    "                    \"direction\": d,       # EB / WB (as in sheet)\n",
    "                    \"Time_Period\": tp,    # AM1..OP1 (as in sheet)\n",
    "                    \"Toll_Rate\": float(val),\n",
    "                }\n",
    "            )\n",
    "\n",
    "rates_long = pd.DataFrame(rows_out, columns=[\"Year\", \"segment\", \"direction\", \"Time_Period\", \"Toll_Rate\"])\n",
    "\n",
    "# Optional final clean-up / typing\n",
    "rates_long[\"Year\"] = rates_long[\"Year\"].astype(\"int64\")\n",
    "rates_long[\"segment\"] = rates_long[\"segment\"].astype(\"int64\")\n",
    "rates_long[\"direction\"] = rates_long[\"direction\"].astype(str).str.upper().str.strip()\n",
    "rates_long[\"Time_Period\"] = rates_long[\"Time_Period\"].astype(str).str.upper().str.strip()\n",
    "rates_long[\"Toll_Rate\"] = rates_long[\"Toll_Rate\"].astype(float)\n",
    "\n",
    "# Quick sanity checks (comment out if not needed)\n",
    "# print(rates_long.head())\n",
    "print(rates_long.groupby([\"Year\", \"direction\"])[\"Time_Period\"].nunique())\n",
    "print(rates_long.pivot_table(index=[\"Year\",\"segment\"], columns=[\"direction\"], values=\"Toll_Rate\", aggfunc=\"count\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef877ac2",
   "metadata": {},
   "source": [
    "# Final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a7277af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_legs: rows=439 ids=437 segments(min,max)=(1,11) directions=['NB', 'SB']\n",
      "CSV files found: 33\n",
      "Will use files:\n",
      "  AMA-I24L3-2024-AM1-SCEN0001-2025-08-04-04-38-50.csv  →  year=2024, period=AM1\n",
      "  AMA-I24L3-2024-AM2-SCEN0001-2025-08-04-04-29-49.csv  →  year=2024, period=AM2\n",
      "  AMA-I24L3-2024-AM3-SCEN0001-2025-08-04-05-21-46.csv  →  year=2024, period=AM3\n",
      "  AMA-I24L3-2024-AM4-SCEN0001-2025-08-04-05-26-52.csv  →  year=2024, period=AM4\n",
      "  AMA-I24L3-2024-AM5-SCEN0001-2025-08-04-05-53-57.csv  →  year=2024, period=AM5\n",
      "  AMA-I24L3-2024-MD1-SCEN0001-2025-08-04-05-49-16.csv  →  year=2024, period=MD1\n",
      "  AMA-I24L3-2024-OP1-SCEN0001-2025-08-04-08-02.csv  →  year=2024, period=OP1\n",
      "  AMA-I24L3-2024-PM1-SCEN0001-2025-08-04-06-36.csv  →  year=2024, period=PM1\n",
      "  AMA-I24L3-2024-PM2-SCEN0001-2025-08-04-07-03.csv  →  year=2024, period=PM2\n",
      "  AMA-I24L3-2024-PM3-SCEN0001-2025-08-04-07-31.csv  →  year=2024, period=PM3\n",
      "  AMA-I24L3-2024-PM4-SCEN0001-2025-08-04-07-55.csv  →  year=2024, period=PM4\n",
      "  AMA-I24L3-2035-AM1-SCEN0001-2025-08-04-09-08.csv  →  year=2035, period=AM1\n",
      "  AMA-I24L3-2035-AM2-SCEN0001-2025-08-04-09-51.csv  →  year=2035, period=AM2\n",
      "  AMA-I24L3-2035-AM3-SCEN0001-2025-08-04-10-27.csv  →  year=2035, period=AM3\n",
      "  AMA-I24L3-2035-AM4-SCEN0001-2025-08-04-11-01.csv  →  year=2035, period=AM4\n",
      "  AMA-I24L3-2035-AM5-SCEN0001-2025-08-04-11-26.csv  →  year=2035, period=AM5\n",
      "  AMA-I24L3-2035-MD1-SCEN0001-2025-08-04-11-45.csv  →  year=2035, period=MD1\n",
      "  AMA-I24L3-2035-OP1-SCEN0001-2025-08-05-02-34.csv  →  year=2035, period=OP1\n",
      "  AMA-I24L3-2035-PM1-SCEN0001-2025-08-05-12-27.csv  →  year=2035, period=PM1\n",
      "  AMA-I24L3-2035-PM2-SCEN0001-2025-08-05-01-16.csv  →  year=2035, period=PM2\n",
      "  AMA-I24L3-2035-PM3-SCEN0001-2025-08-05-01-57.csv  →  year=2035, period=PM3\n",
      "  AMA-I24L3-2035-PM4-SCEN0001-2025-08-05-02-25.csv  →  year=2035, period=PM4\n",
      "  AMA-I24L3-2045-AM1-SCEN0001-2025-08-04-07-55.csv  →  year=2045, period=AM1\n",
      "  AMA-I24L3-2045-AM2-SCEN0001-2025-08-04-08-29.csv  →  year=2045, period=AM2\n",
      "  AMA-I24L3-2045-AM3-SCEN0001-2025-08-04-09-09.csv  →  year=2045, period=AM3\n",
      "  AMA-I24L3-2045-AM4-SCEN0001-2025-08-04-09-41.csv  →  year=2045, period=AM4\n",
      "  AMA-I24L3-2045-AM5-SCEN0001-2025-08-04-10-16.csv  →  year=2045, period=AM5\n",
      "  AMA-I24L3-2045-MD1-SCEN0001-2025-08-04-10-53.csv  →  year=2045, period=MD1\n",
      "  AMA-I24L3-2045-OP1-SCEN0001-2025-08-05-02-18.csv  →  year=2045, period=OP1\n",
      "  AMA-I24L3-2045-PM1-SCEN0001-2025-08-05-12-06.csv  →  year=2045, period=PM1\n",
      "  AMA-I24L3-2045-PM2-SCEN0001-2025-08-05-12-44.csv  →  year=2045, period=PM2\n",
      "  AMA-I24L3-2045-PM3-SCEN0001-2025-08-05-01-31.csv  →  year=2045, period=PM3\n",
      "  AMA-I24L3-2045-PM4-SCEN0001-2025-08-05-01-59.csv  →  year=2045, period=PM4\n",
      "\n",
      "FILE: AMA-I24L3-2024-AM1-SCEN0001-2025-08-04-04-38-50.csv  year=2024 period=AM1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-AM2-SCEN0001-2025-08-04-04-29-49.csv  year=2024 period=AM2 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-AM3-SCEN0001-2025-08-04-05-21-46.csv  year=2024 period=AM3 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-AM4-SCEN0001-2025-08-04-05-26-52.csv  year=2024 period=AM4 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-AM5-SCEN0001-2025-08-04-05-53-57.csv  year=2024 period=AM5 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-MD1-SCEN0001-2025-08-04-05-49-16.csv  year=2024 period=MD1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-OP1-SCEN0001-2025-08-04-08-02.csv  year=2024 period=OP1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-PM1-SCEN0001-2025-08-04-06-36.csv  year=2024 period=PM1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-PM2-SCEN0001-2025-08-04-07-03.csv  year=2024 period=PM2 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-PM3-SCEN0001-2025-08-04-07-31.csv  year=2024 period=PM3 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2024-PM4-SCEN0001-2025-08-04-07-55.csv  year=2024 period=PM4 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-AM1-SCEN0001-2025-08-04-09-08.csv  year=2035 period=AM1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-AM2-SCEN0001-2025-08-04-09-51.csv  year=2035 period=AM2 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-AM3-SCEN0001-2025-08-04-10-27.csv  year=2035 period=AM3 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-AM4-SCEN0001-2025-08-04-11-01.csv  year=2035 period=AM4 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-AM5-SCEN0001-2025-08-04-11-26.csv  year=2035 period=AM5 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-MD1-SCEN0001-2025-08-04-11-45.csv  year=2035 period=MD1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-OP1-SCEN0001-2025-08-05-02-34.csv  year=2035 period=OP1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-PM1-SCEN0001-2025-08-05-12-27.csv  year=2035 period=PM1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-PM2-SCEN0001-2025-08-05-01-16.csv  year=2035 period=PM2 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-PM3-SCEN0001-2025-08-05-01-57.csv  year=2035 period=PM3 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2035-PM4-SCEN0001-2025-08-05-02-25.csv  year=2035 period=PM4 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-AM1-SCEN0001-2025-08-04-07-55.csv  year=2045 period=AM1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-AM2-SCEN0001-2025-08-04-08-29.csv  year=2045 period=AM2 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-AM3-SCEN0001-2025-08-04-09-09.csv  year=2045 period=AM3 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-AM4-SCEN0001-2025-08-04-09-41.csv  year=2045 period=AM4 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-AM5-SCEN0001-2025-08-04-10-16.csv  year=2045 period=AM5 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-MD1-SCEN0001-2025-08-04-10-53.csv  year=2045 period=MD1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-OP1-SCEN0001-2025-08-05-02-18.csv  year=2045 period=OP1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-PM1-SCEN0001-2025-08-05-12-06.csv  year=2045 period=PM1 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-PM2-SCEN0001-2025-08-05-12-44.csv  year=2045 period=PM2 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-PM3-SCEN0001-2025-08-05-01-31.csv  year=2045 period=PM3 rows_in_csv=91548 matched_ids=434\n",
      "\n",
      "FILE: AMA-I24L3-2045-PM4-SCEN0001-2025-08-05-01-59.csv  year=2045 period=PM4 rows_in_csv=91548 matched_ids=434\n",
      "Filled EL Toll_Rate from sheet: 7458\n",
      "GPL nonzero check: True\n",
      "df_all shape: (14388, 8)\n",
      "✅  Saved 14,388 rows → c:\\Git_projects\\I-24-Gantry-TollOptimization\\merged_leg_times.csv\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 2 (final: no toll matching, pick-latest per (year,period)) ───────────\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# Prerequisite: df from Cell 1 with columns: direction | leg | id | Type\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 1) Build df_legs with SEGMENT and normalized ids/directions\n",
    "df_legs = df.copy()\n",
    "df_legs[\"segment\"] = df_legs[\"leg\"].str.extract(r\"leg_(\\d+)\", expand=False)\n",
    "df_legs[\"id\"]      = pd.to_numeric(df_legs[\"id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_legs[\"segment\"] = pd.to_numeric(df_legs[\"segment\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_legs[\"direction\"] = df_legs[\"direction\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(\n",
    "    \"df_legs:\",\n",
    "    f\"rows={len(df_legs)}\",\n",
    "    f\"ids={df_legs['id'].nunique()}\",\n",
    "    f\"segments(min,max)=({df_legs['segment'].min()},{df_legs['segment'].max()})\",\n",
    "    f\"directions={sorted(df_legs['direction'].dropna().unique().tolist())}\"\n",
    ")\n",
    "\n",
    "# 2) Collect candidate CSV files from the three year folders\n",
    "root_dirs = [\n",
    "    r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\ATC_Model_Result\\Input\\Iteration 2 Scenario 1\\2024\",\n",
    "    r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\ATC_Model_Result\\Input\\Iteration 2 Scenario 1\\2035\",\n",
    "    r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\ATC_Model_Result\\Input\\Iteration 2 Scenario 1\\2045\",\n",
    "]\n",
    "all_csv = [p for root in root_dirs for p in Path(root).glob(\"*.csv\")]\n",
    "print(f\"CSV files found: {len(all_csv)}\")\n",
    "\n",
    "# 3) Parse (year, period, stamp) from filename and keep the latest per (year, period)\n",
    "#    Example: AMA-I24L3-2024-PM2-SCEN0001-2025-08-05-01-16.csv\n",
    "import re, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Accepts: AMA-I24L3-2024-AM3-...-2025-08-04-05-21.csv\n",
    "#          AMA-I24L3-2024-AM3-...-2025-08-04-05-21-46.csv\n",
    "pat = re.compile(r\"\"\"\n",
    "^AMA-I24L3-\n",
    " (?P<year>\\d{4})-\n",
    " (?P<period>[A-Z]+[0-9]*)-\n",
    " .*?-\n",
    " (?P<stamp>\\d{4}-\\d{2}-\\d{2}(?:-\\d{2}){2,3})   # HH-MM or HH-MM-SS\n",
    " \\.csv$\n",
    "\"\"\", re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "# Fallback: same but no timestamp in the name\n",
    "pat_no_stamp = re.compile(r\"\"\"\n",
    "^AMA-I24L3-\n",
    " (?P<year>\\d{4})-\n",
    " (?P<period>[A-Z]+[0-9]*)-\n",
    " .*?\n",
    "\\.csv$\n",
    "\"\"\", re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "bucket = {}  # (year, period) -> list[(stamp, src, Path)]\n",
    "\n",
    "for p in all_csv:\n",
    "    m = pat.match(p.name)\n",
    "    if m:\n",
    "        year   = int(m[\"year\"])\n",
    "        period = m[\"period\"]                 # keep EXACT token from filename\n",
    "        stamp  = m[\"stamp\"]                  # 'YYYY-MM-DD-HH-MM' or '...-SS'\n",
    "        src    = \"name\"\n",
    "    else:\n",
    "        m2 = pat_no_stamp.match(p.name)\n",
    "        if not m2:\n",
    "            continue\n",
    "        year   = int(m2[\"year\"])\n",
    "        period = m2[\"period\"]\n",
    "        stamp  = str(int(p.stat().st_mtime)) # fallback to file mtime\n",
    "        src    = \"mtime\"\n",
    "    bucket.setdefault((year, period), []).append((stamp, src, p))\n",
    "\n",
    "files_to_use = []\n",
    "for key, lst in sorted(bucket.items()):\n",
    "    # sort by (use_mtime?, stamp) so real name-stamps sort after mtime if later\n",
    "    lst.sort(key=lambda t: (t[1] != \"name\", t[0]))  # ascending\n",
    "    if len(lst) > 1:\n",
    "        warnings.warn(\n",
    "            f\"Multiple files for (year={key[0]}, period={key[1]}). \"\n",
    "            f\"Using latest: {lst[-1][2].name}\"\n",
    "        )\n",
    "    files_to_use.append((key[0], key[1], lst[-1][2]))\n",
    "\n",
    "\n",
    "print(\"Will use files:\")\n",
    "for y, per, p in files_to_use:\n",
    "    print(f\"  {p.name}  →  year={y}, period={per}\")\n",
    "\n",
    "# 4) Read selected files, compute travel_time, merge with legs\n",
    "id_set = set(df_legs[\"id\"].dropna().unique().tolist())\n",
    "frames = []\n",
    "\n",
    "for year, period, csv_path in files_to_use:\n",
    "    raw = pd.read_csv(csv_path, usecols=[\"ID1\", \"AB_Time\", \"BA_Time\"])\n",
    "    raw[\"ID1\"] = pd.to_numeric(raw[\"ID1\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    before = len(raw)\n",
    "    sub = raw[raw[\"ID1\"].isin(id_set)].copy()\n",
    "    after = len(sub)\n",
    "\n",
    "    print(f\"\\nFILE: {csv_path.name}  year={year} period={period} rows_in_csv={before} matched_ids={after}\")\n",
    "    if sub.empty:\n",
    "        print(\"  → no matching IDs; skipping\")\n",
    "        continue\n",
    "\n",
    "    # travel_time = AB_Time if present else BA_Time\n",
    "    sub[\"travel_time\"] = sub[\"AB_Time\"].fillna(sub[\"BA_Time\"])\n",
    "\n",
    "    sub = (\n",
    "        sub[[\"ID1\", \"travel_time\"]]\n",
    "          .rename(columns={\"ID1\": \"id\"})\n",
    "          .assign(Year=int(year), Time_Period=period, Toll_Rate=0.0)\n",
    "    )\n",
    "\n",
    "    merged = df_legs.merge(sub, on=\"id\", how=\"inner\")\n",
    "    frames.append(merged)\n",
    "\n",
    "# 5) Concatenate and finalize\n",
    "if frames:\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "else:\n",
    "    df_all = pd.DataFrame(columns=[\"Type\",\"Time_Period\",\"segment\",\"Year\",\"direction\",\"id\",\"travel_time\",\"Toll_Rate\"])\n",
    "\n",
    "# Direction and Type normalization\n",
    "df_all[\"direction\"] = (\n",
    "    df_all[\"direction\"].astype(str).str.strip().str.upper()\n",
    "    .replace({\"NB\": \"WB\", \"SB\": \"EB\"})\n",
    ")\n",
    "df_all[\"Type\"] = np.where(\n",
    "    df_all[\"Type\"].astype(str).str.strip().str.upper().eq(\"ML\"),\n",
    "    \"EL\", df_all[\"Type\"]\n",
    ")\n",
    "\n",
    "# Enforce column order and all-zero tolls\n",
    "new_order = [\"Type\",\"Time_Period\",\"segment\",\"Year\",\"direction\",\"id\",\"travel_time\",\"Toll_Rate\"]\n",
    "df_all = df_all[new_order]\n",
    "df_all[\"Toll_Rate\"] = 0.0\n",
    "\n",
    "\n",
    "# normalize keys\n",
    "def _norm(df, include_type=False):\n",
    "    df = df.copy()\n",
    "    # numeric keys\n",
    "    if \"Year\" in df.columns:\n",
    "        df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"segment\" in df.columns:\n",
    "        df[\"segment\"] = pd.to_numeric(df[\"segment\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    # text keys\n",
    "    for col in (\"direction\", \"Time_Period\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.upper().str.strip()\n",
    "    # only df_all has Type\n",
    "    if include_type and \"Type\" in df.columns:\n",
    "        df[\"Type\"] = df[\"Type\"].astype(str).str.upper().str.strip()\n",
    "    return df\n",
    "\n",
    "df_all = _norm(df_all)\n",
    "rates_long = _norm(rates_long)\n",
    "\n",
    "# ensure one rate per key\n",
    "keys = [\"Year\", \"segment\", \"direction\", \"Time_Period\"]\n",
    "rates_long = rates_long.drop_duplicates(subset=keys, keep=\"first\")\n",
    "\n",
    "merged = df_all.merge(\n",
    "    rates_long[keys + [\"Toll_Rate\"]].rename(columns={\"Toll_Rate\": \"Toll_Rate_sheet\"}),\n",
    "    on=keys,\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "# fill only EL rows where Toll_Rate is 0\n",
    "fill_mask = (\n",
    "    merged[\"Type\"].eq(\"EL\") &\n",
    "    merged[\"Toll_Rate\"].fillna(0).eq(0) &\n",
    "    merged[\"Toll_Rate_sheet\"].notna()\n",
    ")\n",
    "merged.loc[fill_mask, \"Toll_Rate\"] = merged.loc[fill_mask, \"Toll_Rate_sheet\"]\n",
    "\n",
    "# enforce GPL = 0, always\n",
    "merged.loc[merged[\"Type\"].eq(\"GPL\"), \"Toll_Rate\"] = 0.0\n",
    "\n",
    "# finalize\n",
    "df_all = merged[[\"Type\",\"Time_Period\",\"segment\",\"Year\",\"direction\",\"id\",\"travel_time\",\"Toll_Rate\"]]\n",
    "\n",
    "print(f\"Filled EL Toll_Rate from sheet: {int(fill_mask.sum())}\")\n",
    "print(\"GPL nonzero check:\", (df_all.query(\"Type=='GPL' and Toll_Rate!=0\").shape[0] == 0))\n",
    "\n",
    "\n",
    "print(\"df_all shape:\", df_all.shape)\n",
    "out_file = Path.cwd() / \"merged_leg_times.csv\"\n",
    "df_all.to_csv(out_file, index=False)\n",
    "print(f\"✅  Saved {len(df_all):,} rows → {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7888f",
   "metadata": {},
   "source": [
    "# update excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8ae1be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote: c:\\Git_projects\\I-24-Gantry-TollOptimization\\output\\AMA-I24L3-TravelTime-GPLvsELComparison-Summary-WithOptimizedTollRates-Iteration 2 Scenario 1-20250806-202023.xlsx\n",
      "✓ Copied to: C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\ATC_Model_Result\\Input\\Iteration 2 Scenario 1\\AMA-I24L3-TravelTime-GPLvsELComparison-Summary-WithOptimizedTollRates-Iteration 2 Scenario 1-20250806-202023.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xlwings as xw\n",
    "import pywintypes  # for safe app.kill() fallback\n",
    "\n",
    "# --- inputs -------------------------------------------------------------------\n",
    "MASTER_XLSX = r\"C:\\Git_projects\\I-24-Gantry-TollOptimization\\AMA-I24L3-TravelTime-GPLvsELComparison-Summary-WithOptimizedTollRates-Master.xlsx\"\n",
    "TARGET_DIR  = r\"C:\\Users\\aligr\\Gradient Systematics, LL\\AMA-I24SL3 - Documents\\06-TDM\\Model Result\\ATC_Model_Result\\Input\\Iteration 2 Scenario 1\"\n",
    "SHEET_NAME  = \"Combined\"\n",
    "TABLE_NAME  = \"TravelTime\"\n",
    "\n",
    "# --- output paths & filename ---------------------------------------------------\n",
    "BASE_NAME = \"AMA-I24L3-TravelTime-GPLvsELComparison-Summary-WithOptimizedTollRates\"\n",
    "scenario_tag = f\"{Path(TARGET_DIR).name}\"  # e.g., \" - Iteration 2 Scenario 1\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # <-- hyphen before hour\n",
    "FILENAME = f\"{BASE_NAME}-{scenario_tag}-{timestamp}.xlsx\"\n",
    "\n",
    "LOCAL_OUT_DIR = Path.cwd() / \"output\"\n",
    "LOCAL_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "Path(TARGET_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOCAL_OUT_PATH = LOCAL_OUT_DIR / FILENAME\n",
    "TARGET_OUT_PATH = Path(TARGET_DIR) / FILENAME\n",
    "\n",
    "# --- 1) Prepare the DataFrame with requested headers --------------------------\n",
    "# Assumes df_all exists with:\n",
    "# [\"Type\",\"Time_Period\",\"segment\",\"Year\",\"direction\",\"id\",\"travel_time\",\"Toll_Rate\"]\n",
    "rename_map = {\n",
    "    \"Type\": \"Type\",\n",
    "    \"Time_Period\": \"Time Period\",\n",
    "    \"segment\": \"Segment\",\n",
    "    \"Year\": \"Year\",\n",
    "    \"direction\": \"Direction\",\n",
    "    \"id\": \"A_B\",\n",
    "    \"travel_time\": \"Time\",\n",
    "    \"Toll Rate\": \"Toll Rate\",   # in case it's already correct\n",
    "    \"Toll_Rate\": \"Toll Rate\",\n",
    "}\n",
    "df_write = df_all.rename(columns=rename_map).loc[:, [\n",
    "    \"Type\", \"Time Period\", \"Segment\", \"Year\", \"Direction\", \"A_B\", \"Time\", \"Toll Rate\"\n",
    "]].copy()\n",
    "\n",
    "# Keep at least one row so ListObject.Resize() always succeeds\n",
    "df_safe = df_write if not df_write.empty else pd.DataFrame(\n",
    "    columns=df_write.columns, data=[[None] * len(df_write.columns)]\n",
    ")\n",
    "\n",
    "# --- 2) Copy master to ./output and edit the copy only ------------------------\n",
    "shutil.copy2(MASTER_XLSX, LOCAL_OUT_PATH)\n",
    "\n",
    "app = xw.App(visible=False, add_book=False)\n",
    "try:\n",
    "    wb = app.books.open(str(LOCAL_OUT_PATH))\n",
    "    sht = wb.sheets[SHEET_NAME]\n",
    "\n",
    "    # Try to get existing ListObject by name\n",
    "    try:\n",
    "        lo = sht.api.ListObjects(TABEL_NAME)  # intentional typo? ensure var name correct\n",
    "    except Exception:\n",
    "        lo = None\n",
    "    # Correct call (keeping above try for safety in some environments)\n",
    "    try:\n",
    "        lo = sht.api.ListObjects(TABLE_NAME)\n",
    "    except Exception:\n",
    "        lo = None\n",
    "\n",
    "    if lo is not None:\n",
    "        # Clear existing table body and refill using the same anchor\n",
    "        try:\n",
    "            # xlwings table wrapper may not always be keyed by name; guard it\n",
    "            try:\n",
    "                body = sht.tables[TABLE_NAME].data_body_range\n",
    "            except Exception:\n",
    "                body = None\n",
    "            if body is not None:\n",
    "                body.clear_contents()\n",
    "\n",
    "            # Fill body (header is already there)\n",
    "            try:\n",
    "                sht.tables[TABLE_NAME].range.offset(1, 0).options(index=False, header=False).value = df_safe\n",
    "            except Exception:\n",
    "                # fallback: write starting at header row + 1, same first column\n",
    "                hdr = lo.HeaderRowRange\n",
    "                top_left = sht.range((hdr.Row + 1, hdr.Column))\n",
    "                top_left.options(index=False, header=False).value = df_safe\n",
    "\n",
    "            # Resize table to exact shape (header + rows, all columns)\n",
    "            hdr = lo.HeaderRowRange\n",
    "            n_rows = int(df_safe.shape[0] + 1)\n",
    "            n_cols = int(df_safe.shape[1])\n",
    "            top_left = sht.range((hdr.Row, hdr.Column))\n",
    "            new_rng = top_left.resize(n_rows, n_cols)\n",
    "            lo.Resize(new_rng.api)\n",
    "        except Exception:\n",
    "            # If anything fails, rebuild cleanly\n",
    "            lo.Delete()\n",
    "            lo = None\n",
    "\n",
    "    if lo is None:\n",
    "        # Rebuild table from scratch at A1\n",
    "        sht.clear()\n",
    "        top_left = sht.range(\"A1\")\n",
    "        top_left.options(index=False, header=True).value = df_safe\n",
    "        rng = top_left.expand()\n",
    "        lo = sht.api.ListObjects.Add(1, rng.api, None, 1)  # xlSrcRange=1, HasHeaders=1\n",
    "        lo.Name = TABLE_NAME\n",
    "\n",
    "        # Ensure exact size\n",
    "        hdr = lo.HeaderRowRange\n",
    "        n_rows = int(df_safe.shape[0] + 1)\n",
    "        n_cols = int(df_safe.shape[1])\n",
    "        top_left = sht.range((hdr.Row, hdr.Column))\n",
    "        new_rng = top_left.resize(n_rows, n_cols)\n",
    "        lo.Resize(new_rng.api)\n",
    "\n",
    "    # Optional: refresh connections/pivots\n",
    "    try:\n",
    "        wb.api.RefreshAll()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    wb.save()  # saves the edited copy in ./output/\n",
    "finally:\n",
    "    try:\n",
    "        app.quit()\n",
    "    except pywintypes.com_error:\n",
    "        app.kill()\n",
    "\n",
    "# --- 3) Copy finalized file to target directory --------------------------------\n",
    "shutil.copy2(LOCAL_OUT_PATH, TARGET_OUT_PATH)\n",
    "\n",
    "print(f\"✓ Wrote: {LOCAL_OUT_PATH}\")\n",
    "print(f\"✓ Copied to: {TARGET_OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0fc0a",
   "metadata": {},
   "source": [
    "# Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--venv-dir VENV_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\aligr\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3e69e41f080885d8a1bf197553c77f69eff1cf197.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git_projects\\AMA-I24L3-TravelTime-GPLvsELComparison-Summary-WithOptimizedTollRates\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Bootstrap-and-push helper.\n",
    "\n",
    "Works seamlessly when:\n",
    "  • executed from a .py file   →  python git_push.py --venv-dir .venv\n",
    "  • invoked inside Jupyter     →  %run git_push.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, subprocess, argparse, textwrap\n",
    "\n",
    "# ── CONFIG ──────────────────────────────────────────────────────────────────\n",
    "REPO_URL          = \"https://github.com/gradient-systematics/AMA-I24L3-TravelTime-GPLvsELComparison-Summary-WithOptimizedTollRates\"\n",
    "DEFAULT_VENV_DIR  = \".venv\"\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def in_notebook() -> bool:\n",
    "    \"\"\"Return True if running inside IPython/Jupyter.\"\"\"\n",
    "    try:\n",
    "        from IPython import get_ipython          # lazy import\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def run(cmd: list[str], **kw):\n",
    "    \"\"\"Run a subprocess command, exit on error.\"\"\"\n",
    "    subprocess.run(cmd, check=True, **kw)\n",
    "\n",
    "def find_pip(venv_dir: str) -> str | None:\n",
    "    \"\"\"Locate pip executable inside a venv.\"\"\"\n",
    "    paths = [\n",
    "        os.path.join(venv_dir, \"Scripts\", \"pip.exe\"),   # Windows\n",
    "        os.path.join(venv_dir, \"Scripts\", \"pip\"),\n",
    "        os.path.join(venv_dir, \"bin\",     \"pip\"),       # POSIX\n",
    "    ]\n",
    "    return next((p for p in paths if os.path.isfile(p)), None)\n",
    "\n",
    "def create_gitignore(venv_dir: str):\n",
    "    ignore = textwrap.dedent(f\"\"\"\\\n",
    "        # Office & data files\n",
    "        *.xls\n",
    "        *.xlsx\n",
    "        *.xlsm\n",
    "        *.csv\n",
    "        *.doc*\n",
    "        *.ppt*\n",
    "        *.pdf\n",
    "        *.html\n",
    "        \n",
    "\n",
    "        # Build/output artefacts\n",
    "        /output/\n",
    "\n",
    "        # Virtual environments\n",
    "        {venv_dir}/\n",
    "        env/\n",
    "        venv/\n",
    "\n",
    "        # Python cache\n",
    "        __pycache__/\n",
    "        *.py[cod]\n",
    "    \"\"\")\n",
    "    with open(\".gitignore\", \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(ignore + \"\\n\")\n",
    "    print(\"✔ .gitignore created\")\n",
    "\n",
    "def main(argv: list[str] | None = None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Initialize git repo, push, and dump requirements.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--venv-dir\",\n",
    "        default=DEFAULT_VENV_DIR,\n",
    "        help=\"Name or path of your virtual-env folder (default: .venv)\",\n",
    "    )\n",
    "\n",
    "    # NOTE: parse_known_args keeps going even if Jupyter adds --f=<json>\n",
    "    args, _ = parser.parse_known_args(argv)\n",
    "    venv_dir = args.venv_dir\n",
    "\n",
    "    create_gitignore(venv_dir)\n",
    "\n",
    "    # Git init & remote\n",
    "    run([\"git\", \"init\"])\n",
    "    subprocess.run([\"git\", \"remote\", \"remove\", \"origin\"], check=False)\n",
    "    run([\"git\", \"remote\", \"add\", \"origin\", REPO_URL])\n",
    "\n",
    "    # First commit & push\n",
    "    run([\"git\", \"add\", \".\"])\n",
    "    run([\"git\", \"commit\", \"-m\", \"Initial commit\"])\n",
    "    run([\"git\", \"branch\", \"-M\", \"main\"])\n",
    "    run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])\n",
    "    print(f\"✔ Code pushed to {REPO_URL}\")\n",
    "\n",
    "    # requirements.txt\n",
    "    pip_exe = find_pip(venv_dir)\n",
    "    if pip_exe:\n",
    "        result = subprocess.run([pip_exe, \"freeze\"],\n",
    "                                check=True,\n",
    "                                stdout=subprocess.PIPE,\n",
    "                                text=True)\n",
    "        with open(\"requirements.txt\", \"w\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(result.stdout)\n",
    "        print(f\"✔ requirements.txt generated from {venv_dir}\")\n",
    "    else:\n",
    "        print(f\"⚠  Could not find pip inside {venv_dir}; skipped requirements.txt\")\n",
    "\n",
    "    print(\"All done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Prevent disruptive SystemExit traceback spam inside notebooks\n",
    "    try:\n",
    "        main()\n",
    "    except SystemExit as e:\n",
    "        if in_notebook():\n",
    "            print(f\"[git_push.py exited with status {e.code}]\")\n",
    "        else:\n",
    "            raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
